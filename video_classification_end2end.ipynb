{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../pytorchvideo\")\n",
    "from pytorchvideo.data import LabeledVideoDataset,Kinetics, make_clip_sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/datasets/mohamedmustafa/real-life-violence-situations-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T20:18:04.995677Z",
     "iopub.status.busy": "2022-09-15T20:18:04.995237Z",
     "iopub.status.idle": "2022-09-15T20:18:06.789586Z",
     "shell.execute_reply": "2022-09-15T20:18:06.788360Z",
     "shell.execute_reply.started": "2022-09-15T20:18:04.995639Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pytorch_lightning import seed_everything, LightningModule, Trainer\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint,LearningRateMonitor\n",
    "from torch.optim.lr_scheduler import CyclicLR, ReduceLROnPlateau,CosineAnnealingWarmRestarts,OneCycleLR,CosineAnnealingLR\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset,ConcatDataset,default_collate\n",
    "from sklearn.model_selection import KFold,GroupShuffleSplit,GroupKFold,LeaveOneGroupOut\n",
    "from torchmetrics import MeanAbsoluteError\n",
    "from sklearn.utils import shuffle\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchaudio import transforms as TA\n",
    "from sklearn.metrics import classification_report\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T20:18:06.794066Z",
     "iopub.status.busy": "2022-09-15T20:18:06.792689Z",
     "iopub.status.idle": "2022-09-15T20:18:06.803825Z",
     "shell.execute_reply": "2022-09-15T20:18:06.802108Z",
     "shell.execute_reply.started": "2022-09-15T20:18:06.794021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0+cu113'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "non=glob('NonViolence/*')\n",
    "vio=glob('Violence/*')\n",
    "label=[0]*len(non)+[1]*len(vio)\n",
    "df=pd.DataFrame(zip(non+vio,label),columns=['file','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,val_df=train_test_split(df,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T20:18:06.872186Z",
     "iopub.status.busy": "2022-09-15T20:18:06.871763Z",
     "iopub.status.idle": "2022-09-15T20:18:06.889484Z",
     "shell.execute_reply": "2022-09-15T20:18:06.888400Z",
     "shell.execute_reply.started": "2022-09-15T20:18:06.872133Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/talha/venv/lib/python3.8/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/talha/venv/lib/python3.8/site-packages/torchvision/transforms/_transforms_video.py:25: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pytorchvideo.data import LabeledVideoDataset,Kinetics, make_clip_sampler\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "#     RemoveKey,\n",
    "#     ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    Permute\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize\n",
    ")\n",
    "\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T20:18:06.892088Z",
     "iopub.status.busy": "2022-09-15T20:18:06.891199Z",
     "iopub.status.idle": "2022-09-15T20:18:06.897433Z",
     "shell.execute_reply": "2022-09-15T20:18:06.896290Z",
     "shell.execute_reply.started": "2022-09-15T20:18:06.892051Z"
    }
   },
   "outputs": [],
   "source": [
    "#tuneable params\n",
    "num_video_samples=20\n",
    "video_duration=2\n",
    "model_name='efficient_x3d_xs'\n",
    "batch_size=8\n",
    "scheduler='cosine'\n",
    "clipmode='random'\n",
    "img_size=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T20:18:06.901622Z",
     "iopub.status.busy": "2022-09-15T20:18:06.900753Z",
     "iopub.status.idle": "2022-09-15T20:18:06.910916Z",
     "shell.execute_reply": "2022-09-15T20:18:06.909562Z",
     "shell.execute_reply.started": "2022-09-15T20:18:06.901576Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorchvideo.data import LabeledVideoDataset, make_clip_sampler,labeled_video_dataset\n",
    "from torchvision.transforms import ColorJitter,RandomAdjustSharpness,RandomAutocontrast\n",
    "video_transform = Compose(\n",
    "            [\n",
    "            ApplyTransformToKey(\n",
    "              key=\"video\",\n",
    "              transform=Compose(\n",
    "                  [\n",
    "                    UniformTemporalSubsample(num_video_samples),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                      #Determines the shorter spatial dim of the video (i.e. width or height) and scales it to the given size\n",
    "                    RandomShortSideScale(min_size=img_size+16, max_size=img_size+32),\n",
    "                    CenterCropVideo(img_size),\n",
    "                    RandomHorizontalFlip(p=0.5),\n",
    "                  ]\n",
    "                ),\n",
    "              ),\n",
    "            ]\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T20:18:06.924208Z",
     "iopub.status.busy": "2022-09-15T20:18:06.923758Z",
     "iopub.status.idle": "2022-09-15T20:18:06.958966Z",
     "shell.execute_reply": "2022-09-15T20:18:06.955103Z",
     "shell.execute_reply.started": "2022-09-15T20:18:06.924171Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset=labeled_video_dataset(val_df,\n",
    "                   clip_sampler=make_clip_sampler(clipmode, video_duration),\\\n",
    "                    transform=video_transform, decode_audio=False\n",
    "                                   )\n",
    "        \n",
    "train_loader=DataLoader(train_dataset,batch_size=4,\n",
    "           num_workers=0,\n",
    "           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T20:18:06.971415Z",
     "iopub.status.busy": "2022-09-15T20:18:06.970920Z",
     "iopub.status.idle": "2022-09-15T20:18:12.962845Z",
     "shell.execute_reply": "2022-09-15T20:18:12.961746Z",
     "shell.execute_reply.started": "2022-09-15T20:18:06.971365Z"
    }
   },
   "outputs": [],
   "source": [
    "batch=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T20:18:12.964898Z",
     "iopub.status.busy": "2022-09-15T20:18:12.964470Z",
     "iopub.status.idle": "2022-09-15T20:18:12.972822Z",
     "shell.execute_reply": "2022-09-15T20:18:12.971638Z",
     "shell.execute_reply.started": "2022-09-15T20:18:12.964831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 20, 224, 224]), torch.Size([4, 1]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['video'].shape,batch['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T20:18:12.975981Z",
     "iopub.status.busy": "2022-09-15T20:18:12.975072Z",
     "iopub.status.idle": "2022-09-15T20:18:13.003089Z",
     "shell.execute_reply": "2022-09-15T20:18:13.001856Z",
     "shell.execute_reply.started": "2022-09-15T20:18:12.975944Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import torchvision.models as models\n",
    "import timm\n",
    "class OurModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(OurModel,self).__init__()\n",
    "\n",
    "        self.scheduler=scheduler\n",
    "\n",
    "        \n",
    "        self.video_model = torch.hub.load('facebookresearch/pytorchvideo', model_name, pretrained=True)\n",
    "        self.video_model.projection.model=nn.Linear(in_features=2048, out_features=1000, bias=True)\n",
    "        \n",
    "       \n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear=nn.Linear(1000,1)\n",
    "        \n",
    "        self.lr=1e-3\n",
    "        self.batch_size=batch_size\n",
    "        self.numworker=6\n",
    "        \n",
    "        self.metric = torchmetrics.Accuracy()\n",
    "        self.criterion=nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self,video):\n",
    "        x=self.video_model(video)\n",
    "        x=self.relu(x)\n",
    "        x=self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt=torch.optim.AdamW(params=self.parameters(),lr=self.lr )\n",
    "        if self.scheduler=='cosine':\n",
    "            scheduler=CosineAnnealingLR(opt,T_max=10,  eta_min=1e-6, last_epoch=-1)\n",
    "            return {'optimizer': opt,'lr_scheduler':scheduler}\n",
    "        elif self.scheduler=='reduce':\n",
    "            scheduler=ReduceLROnPlateau(opt,mode='min', factor=0.5, patience=5)\n",
    "            return {'optimizer': opt,'lr_scheduler':scheduler,'monitor':'val_loss'}\n",
    "        elif self.scheduler=='warm':\n",
    "            scheduler=CosineAnnealingWarmRestarts(opt,T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1)\n",
    "            return {'optimizer': opt,'lr_scheduler':scheduler}\n",
    "        elif self.scheduler=='cycle':\n",
    "            opt=torch.optim.AdamW(params=self.parameters(),lr=1e-6 )\n",
    "            scheduler=OneCycleLR(opt,max_lr=1e-2,epochs=15,steps_per_epoch=len(self.train_df)//self.batch_size//4)\n",
    "            lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n",
    "            return {'optimizer': opt, 'lr_scheduler': lr_scheduler}\n",
    "        elif self.scheduler=='lambda':\n",
    "            lambda1 = lambda epoch: 0.9 ** epoch\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lambda1)\n",
    "            return {'optimizer': opt, 'lr_scheduler': scheduler}\n",
    "        elif self.scheduler=='constant':\n",
    "            return opt\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        dataset=labeled_video_dataset(train_df,\n",
    "                   clip_sampler=make_clip_sampler(clipmode, video_duration),\\\n",
    "                    transform=video_transform, decode_audio=False)\n",
    "        \n",
    "        loader=DataLoader(dataset,batch_size=self.batch_size,\n",
    "                   num_workers=self.numworker,\n",
    "                   pin_memory=True)\n",
    "        return loader\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        video,label=batch['video'],batch['label']\n",
    "#         label=label.ravel().to(torch.int64)\n",
    "        out = self(video)\n",
    "        loss=self.criterion(out,label)\n",
    "        metric=self.metric(out,label.to(torch.int64))\n",
    "        return {'loss':loss,'metric':metric.detach()}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        loss=torch.stack([x[\"loss\"] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        metric=torch.stack([x[\"metric\"] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        self.log('train_loss', loss,batch_size=self.batch_size)\n",
    "        self.log('train_metric', metric,batch_size=self.batch_size)\n",
    "        print('training loss ',self.current_epoch,loss,metric)\n",
    "   \n",
    "    def val_dataloader(self):\n",
    "        dataset=labeled_video_dataset(val_df,\n",
    "                   clip_sampler=make_clip_sampler(clipmode, video_duration),\\\n",
    "                    transform=video_transform, decode_audio=False)\n",
    "        \n",
    "        loader=DataLoader(dataset,batch_size=self.batch_size,\n",
    "                   num_workers=self.numworker,\n",
    "                   pin_memory=True)\n",
    "        return loader\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        video,label=batch['video'],batch['label']\n",
    "        out = self(video)\n",
    "        loss=self.criterion(out,label)\n",
    "        metric=self.metric(out,label.to(torch.int64))\n",
    "        return {'loss':loss,'metric':metric.detach()}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss=torch.stack([x[\"loss\"] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        metric=torch.stack([x[\"metric\"] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        print('validation loss ',self.current_epoch,loss,metric)\n",
    "        self.log('val_loss', loss,batch_size=self.batch_size)\n",
    "        self.log('val_metric',metric,batch_size=self.batch_size)\n",
    "   \n",
    "    def test_dataloader(self):\n",
    "        dataset=labeled_video_dataset(val_df,\n",
    "                   clip_sampler=make_clip_sampler(clipmode, video_duration),\\\n",
    "                    transform=video_transform, decode_audio=False)\n",
    "        \n",
    "        loader=DataLoader(dataset,batch_size=self.batch_size,\n",
    "                   num_workers=self.numworker,\n",
    "                   pin_memory=True)\n",
    "        return loader\n",
    "\n",
    "  \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        video,label=batch['video'],batch['label']\n",
    "        out = self(video)\n",
    "        return { 'label': label.detach(), 'pred': out.detach()}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        label = torch.cat([x['label'] for x in outputs]).cpu().numpy()\n",
    "        pred = torch.cat([x['pred'] for x in outputs]).cpu().numpy()\n",
    "        pred=np.where(pred>0.5,1,0)\n",
    "        print(classification_report(label, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-15T20:18:13.004769Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss',dirpath='checkpoints',\n",
    "                                        filename='file',save_last=True)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(project=\"violence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/talha/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "Global seed set to 0\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model=OurModel()\n",
    "seed_everything(0)\n",
    "trainer = Trainer(max_epochs=30, \n",
    "#                 deterministic=True,\n",
    "                accelerator='gpu', devices=-1,\n",
    "                  precision=16,\n",
    "                accumulate_grad_batches=2,\n",
    "                enable_progress_bar = False,\n",
    "                num_sanity_val_steps=0,\n",
    "                  callbacks=[lr_monitor,checkpoint_callback],\n",
    "#                 limit_train_batches=5,\n",
    "#                 limit_val_batches=1,\n",
    "#                 logger=wandb_logger\n",
    "\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /media/talha/data/image/classification/video_classification/Real Life Violence Dataset/lightning_logs\n",
      "/home/talha/venv/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /media/talha/data/image/classification/video_classification/Real Life Violence Dataset/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Restoring states from the checkpoint path at checkpoints/last.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type              | Params\n",
      "--------------------------------------------------\n",
      "0 | video_model | EfficientX3d      | 5.0 M \n",
      "1 | relu        | ReLU              | 0     \n",
      "2 | linear      | Linear            | 1.0 K \n",
      "3 | metric      | Accuracy          | 0     \n",
      "4 | criterion   | BCEWithLogitsLoss | 0     \n",
      "--------------------------------------------------\n",
      "5.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.0 M     Total params\n",
      "10.049    Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint file at checkpoints/last.ckpt\n",
      "2022-09-26 15:57:11.460252: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model,\n",
    "#             ckpt_path='checkpoints/last.ckpt'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_res=trainer.validate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger.experiment.save('notebook.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
